{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will perform **Bayesian Optimization** with Gaussian Processes in Parallel, utilizing various CPUs, to speed up the search.\n",
    "\n",
    "This is useful to reduce search times. \n",
    "\n",
    "https://scikit-optimize.github.io/stable/auto_examples/parallel-optimization.html#example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 120, name=\"n_estimators\"),\n",
    "    Integer(1, 5, name=\"max_depth\"),\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),\n",
    "    Categorical(['deviance', 'exponential'], name=\"loss\"),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement for scikit-optimize.\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Optimizer\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions = param_grid, # the hyperparameter space\n",
    "    base_estimator = \"GP\", # the surrogate\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    random_state=0, \n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    }
   ],
   "source": [
    "# we will use 4 CPUs (n_points)\n",
    "# if we loop 10 times using 4 end points, we perform 40 searches in total\n",
    "\n",
    "for i in range(10):\n",
    "    x = optimizer.ask(n_points=4)  # x is a list of n_points points\n",
    "    y = Parallel(n_jobs=4)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "    optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 4, 0.007381238832487747, 0.08704800719052391, 'exponential'],\n",
       " [118, 2, 0.00010113718979245275, 0.05725990689319986, 'deviance'],\n",
       " [17, 3, 0.00022221129238269847, 0.17371729808265857, 'exponential'],\n",
       " [42, 4, 0.000960176974739521, 0.6793527084375192, 'exponential'],\n",
       " [38, 5, 0.053126979002083165, 0.06277193933628669, 'deviance'],\n",
       " [28, 4, 0.005445788189169609, 0.002258808366805843, 'deviance'],\n",
       " [56, 1, 0.0006780193306440557, 0.9274466173670369, 'exponential'],\n",
       " [42, 4, 0.08952334707464486, 0.20644568894340298, 'deviance'],\n",
       " [68, 1, 0.0010637763908757617, 0.0037524397848558923, 'deviance'],\n",
       " [48, 4, 0.0016815858162959685, 0.27886812907463643, 'deviance'],\n",
       " [107, 5, 0.016812145780053037, 0.008608554188871567, 'exponential'],\n",
       " [10, 2, 0.05193389864280579, 0.2421867730253962, 'exponential'],\n",
       " [25, 3, 0.07191463533071733, 0.0922143588021794, 'deviance'],\n",
       " [120, 5, 0.07697017752329953, 0.8551443380247231, 'deviance'],\n",
       " [10, 2, 0.07842467548659834, 0.0011887833627052004, 'deviance'],\n",
       " [10, 4, 0.0812161158863383, 0.999, 'deviance'],\n",
       " [120, 1, 0.07773028928910658, 0.001, 'deviance'],\n",
       " [120, 1, 0.07607563981764849, 0.001, 'deviance'],\n",
       " [120, 1, 0.07630178955634367, 0.034960023811925194, 'deviance'],\n",
       " [114, 5, 0.07804002036321836, 0.001, 'deviance'],\n",
       " [120, 3, 0.07894625945850003, 0.999, 'deviance'],\n",
       " [120, 1, 0.07901034902587137, 0.999, 'deviance'],\n",
       " [120, 5, 0.1, 0.999, 'exponential'],\n",
       " [120, 2, 0.017801265719156197, 0.940702307149603, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [10, 1, 0.1, 0.001, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [64, 1, 0.02575770402834184, 0.001, 'deviance'],\n",
       " [71, 5, 0.018167319709670702, 0.001, 'deviance'],\n",
       " [74, 1, 0.01943622799156283, 0.999, 'exponential'],\n",
       " [120, 5, 0.004969136791378505, 0.999, 'exponential'],\n",
       " [120, 5, 0.03559723381947727, 0.999, 'deviance'],\n",
       " [120, 4, 0.05311655470323014, 0.001, 'exponential'],\n",
       " [120, 2, 0.039471805843425696, 0.03632177939374477, 'exponential'],\n",
       " [89, 1, 0.007794483623478603, 0.002690253278428203, 'exponential'],\n",
       " [46, 1, 0.055556136582551485, 0.001, 'exponential'],\n",
       " [57, 1, 0.1, 0.001, 'deviance'],\n",
       " [65, 1, 0.06801769971681054, 0.999, 'deviance'],\n",
       " [48, 1, 0.06309502343448085, 0.999, 'deviance']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the evaluated hyperparamters\n",
    "\n",
    "optimizer.Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9171413381939697,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9296536796536796,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9347231715652767,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9221348826611985,\n",
       " -0.9146350725298094,\n",
       " -0.9321979190400244,\n",
       " -0.9673425989215462,\n",
       " -0.9246601351864511,\n",
       " -0.8994645705172021,\n",
       " -0.9497607655502392,\n",
       " -0.9472355130249867,\n",
       " -0.9472355130249867,\n",
       " -0.9271664008506114,\n",
       " -0.9497607655502392,\n",
       " -0.9497607655502392,\n",
       " -0.9572795625427205,\n",
       " -0.9346662109820004,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9170274170274171,\n",
       " -0.9572795625427205,\n",
       " -0.9296536796536796,\n",
       " -0.9196476038581302,\n",
       " -0.9271094402673349,\n",
       " -0.8944330523277891,\n",
       " -0.9497607655502392,\n",
       " -0.9322169059011164,\n",
       " -0.942241968557758,\n",
       " -0.8994645705172021,\n",
       " -0.9472544998860788,\n",
       " -0.9472355130249867,\n",
       " -0.9497607655502392,\n",
       " -0.9472544998860788]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the accuracy\n",
    "\n",
    "optimizer.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "0            68          4           0.007381       0.087048  exponential   \n",
       "1           118          2           0.000101       0.057260     deviance   \n",
       "2            17          3           0.000222       0.173717  exponential   \n",
       "3            42          4           0.000960       0.679353  exponential   \n",
       "4            38          5           0.053127       0.062772     deviance   \n",
       "\n",
       "   accuracy  \n",
       "0 -0.917141  \n",
       "1 -0.625636  \n",
       "2 -0.625636  \n",
       "3 -0.625636  \n",
       "4 -0.929654  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together in one dataframe, so we can investigate further\n",
    "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
    "\n",
    "tmp = pd.concat([\n",
    "    pd.DataFrame(optimizer.Xi),\n",
    "    pd.Series(optimizer.yi),\n",
    "], axis=1)\n",
    "\n",
    "tmp.columns = dim_names + ['accuracy']\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6G0lEQVR4nO3deXxU9b3/8fdMliGBJEMgYQgEIaIsCmErMUitNflhwLrVW8XGBS83uIDWQFtDW6vYW6F1aQv11vpzqf6ExqVSq9dSorgUCAmkBpCGqBFkS4gakxBChknm/P4IGYwkhJA5cybD6/l4zKPJmXPOfL490Xn7/X7P99gMwzAEAAAQQuxWFwAAAOBvBBwAABByCDgAACDkEHAAAEDIIeAAAICQQ8ABAAAhh4ADAABCDgEHAACEnHCrC7CC1+vVgQMHFBMTI5vNZnU5AADgFBiGoUOHDikpKUl2+8n7aM7IgHPgwAElJydbXQYAADgNe/fu1dChQ0+6zxkZcGJiYiS1/h8UGxtrcTUAAOBU1NfXKzk52fc9fjJnZMBpG5aKjY0l4AAA0MucyvQSJhkDAICQQ8ABAAAhh4ADAABCDgEHAACEHAIOAAAIOQQcAAAQcgg4AAAg5BBwAABAyCHgAACAkEPAAQAAIYeAAwAAQg4BBwAAhJwz8mGbZin5tEavb6u0uoyg9I3h8Zo1brDVZQAAzhAEHD8qr2rQMxt2W11GUPp/hZ/q4lEJio7kTw4AYD6+bfzovKRYzf/22VaXEXT+8E6Fmr2GDjU1E3AAAAHBt40fpSY7lZrstLqMoPPsxk/V4G5W49EWq0sBAJwhmGQM00VFhkmSGo82W1wJAOBMQcCB6aKPBZwj9OAAAAKEgAPTtc27OUzAAQAECAEHpjveg8MQFQAgMAg4MF20bw4OPTgAgMAg4MB0UREEHABAYBFwYDomGQMAAo2AA9NFO9omGTMHBwAQGAQcmC46gh4cAEBgEXBgOiYZAwACjYAD00UdWweHgAMACBQCDkznm2TsYQ4OACAwCDgwXVvAOeymBwcAEBgEHJiu7VENTDIGAAQKAQem800yZogKABAgBByYLoq7qAAAAUbAgelYyRgAEGimBZyamhplZ2crNjZWTqdTc+fOVUNDQ5fHFRYW6pJLLlHfvn0VGxuriy66SEeOHPG9P3z4cNlstnavZcuWmdUM+EHbHJzDboaoAACBEW7WibOzs1VZWamCggJ5PB7dcsstmjdvnlatWtXpMYWFhcrKytLixYu1YsUKhYeHa+vWrbLb2+ewBx54QDk5Ob7fY2JizGoG/OD4beL04AAAAsOUgFNWVqY1a9Zo8+bNmjJliiRpxYoVmjVrlh5++GElJSV1eFxubq7uuusu5eXl+baNGjXqhP1iYmLkcrnMKB0maAs4nhZDnhavIsIYGQUAmMuUb5rCwkI5nU5fuJGkzMxM2e12FRUVdXhMdXW1ioqKlJiYqGnTpmnQoEH61re+pfXr15+w77JlyzRgwABNnDhRDz30kJqbTz704Xa7VV9f3+6FwGmbZCwx0RgAEBimBJyqqiolJia22xYeHq74+HhVVVV1eMwnn3wiSbr//vuVk5OjNWvWaNKkScrIyNBHH33k2++uu+5Sfn6+3n77bd1666168MEH9eMf//ik9SxdulRxcXG+V3Jycg9biO6IDLMrzG6TxERjAEBgdCvg5OXlnTDB9+uvnTt3nlYhXq9XknTrrbfqlltu0cSJE/Wb3/xGo0aN0tNPP+3bb+HChbr44os1fvx43XbbbXrkkUe0YsUKud3uTs+9ePFi1dXV+V579+49rRpxemw22/HVjI8y0RgAYL5uzcFZtGiR5syZc9J9UlJS5HK5VF1d3W57c3OzampqOp07M3jwYEnS2LFj220fM2aM9uzZ0+nnpaWlqbm5Wbt37+5wvo4kORwOORyOk9YNc0VHhulQUzM9OACAgOhWwElISFBCQkKX+6Wnp6u2tlYlJSWaPHmyJGndunXyer1KS0vr8Jjhw4crKSlJ5eXl7bZ/+OGHmjlzZqefVVpaKrvdfsKQGIJL663ibubgAAACwpS7qMaMGaOsrCzl5OTo8ccfl8fj0YIFCzR79mzfHVT79+9XRkaGnnvuOU2dOlU2m00/+tGPdN999yk1NVUTJkzQs88+q507d+rll1+W1Dp5uaioSN/+9rcVExOjwsJC5ebm6oYbblD//v3NaAr8JCqibTVjhqgAAOYzbR2clStXasGCBcrIyJDdbtc111yj5cuX+973eDwqLy9XY2Ojb9vdd9+tpqYm5ebmqqamRqmpqSooKNDZZ58tqXWoKT8/X/fff7/cbrdGjBih3NxcLVy40KxmwE9YzRgAEEg2wzAMq4sItPr6esXFxamurk6xsbFWl3NGuOnpYr334Wd6+Hup+o/JQ60uBwDQC3Xn+5sV1xAQ0RFtPTgMUQEAzEfAQUBE80RxAEAAEXAQEFEEHABAABFwEBA8cBMAEEgEHARE6zo40mE3c3AAAOYj4CAguE0cABBIBBwEBJOMAQCBRMBBQEQdG6JqZA4OACAACDgIiONDVMzBAQCYj4CDgGgLOIfd9OAAAMxHwEFAtN1FxW3iAIBAIOAgII5PMmaICgBgPgIOAoKVjAEAgUTAQUCwDg4AIJAIOAiItjk4zV5DR5u9FlcDAAh1BBwERFsPjsQ8HACA+Qg4CIiIMLsiwmySmIcDADAfAQcBExXBRGMAQGAQcBAwvrVwCDgAAJMRcBAw0Q7WwgEABAYBBwHDE8UBAIFCwEHAREcce6I4AQcAYDICDgImisc1AAAChICDgPGtZswDNwEAJiPgIGDa7qJiiAoAYDYCDgLGN8nYzRAVAMBcBBwEDHdRAQAChYCDgPFNMmYODgDAZAQcBIxvkjE9OAAAkxFwEDDHJxkzBwcAYC4CDgKGOTgAgEAh4CBgCDgAgEAh4CBgolgHBwAQIAQcBMzxScbMwQEAmIuAg4BhiAoAECgEHAQMj2oAAAQKAQcBE/2Vp4kbhmFxNQCAUEbAQcC0rWTsNSR3s9fiagAAoczUgFNTU6Ps7GzFxsbK6XRq7ty5amho6HT/3bt3y2azdfh66aWXfPvt2bNHl112maKjo5WYmKgf/ehHam5m4mqwi44I8/3MasYAADOFm3ny7OxsVVZWqqCgQB6PR7fccovmzZunVatWdbh/cnKyKisr22174okn9NBDD2nmzJmSpJaWFl122WVyuVzauHGjKisrddNNNykiIkIPPvigmc1BD4WH2RUZbtfRZq8aPS3qb3VBAICQZTNMmgxRVlamsWPHavPmzZoyZYokac2aNZo1a5b27dunpKSkUzrPxIkTNWnSJD311FOSpL///e/6zne+owMHDmjQoEGSpMcff1z33HOPPvvsM0VGRnZ5zvr6esXFxamurk6xsbGn2UKcjgkPrFVto0cFuRfpnEExVpcDAOhFuvP9bdoQVWFhoZxOpy/cSFJmZqbsdruKiopO6RwlJSUqLS3V3Llz25133LhxvnAjSZdeeqnq6+u1Y8cO/zUApmgbpuJOKgCAmUwboqqqqlJiYmL7DwsPV3x8vKqqqk7pHE899ZTGjBmjadOmtTvvV8ONJN/vnZ3X7XbL7Xb7fq+vrz+lz4f/RbEWDgAgALrdg5OXl9fpROC2186dO3tc2JEjR7Rq1ap2vTena+nSpYqLi/O9kpOTe3xOnJ62tXCOeJgUDgAwT7d7cBYtWqQ5c+acdJ+UlBS5XC5VV1e3297c3Kyamhq5XK4uP+fll19WY2OjbrrppnbbXS6XiouL2207ePCg772OLF68WAsXLvT9Xl9fT8ixCKsZAwACodsBJyEhQQkJCV3ul56ertraWpWUlGjy5MmSpHXr1snr9SotLa3L45966ildccUVJ3xWenq6fvnLX6q6uto3BFZQUKDY2FiNHTu2w3M5HA45HI4uPxPm8wUcNwEHAGAe0yYZjxkzRllZWcrJyVFxcbE2bNigBQsWaPbs2b47qPbv36/Ro0ef0CPz8ccf67333tN//dd/nXDeGTNmaOzYsbrxxhu1detW/eMf/9DPfvYzzZ8/nxDTCxx/XANDVAAA85i60N/KlSs1evRoZWRkaNasWZo+fbqeeOIJ3/sej0fl5eVqbGxsd9zTTz+toUOHasaMGSecMywsTK+//rrCwsKUnp6uG264QTfddJMeeOABM5sCP/FNMvbQgwMAMI9p6+AEM9bBsc7PX/1AzxV+qjsvGalFM0ZZXQ4AoBcJinVwgI7wRHEAQCAQcBBQX32iOAAAZiHgIKC4TRwAEAgEHAQUKxkDAAKBgIOAauvBOULAAQCYiICDgGIdHABAIBBwEFDMwQEABAIBBwFFwAEABAIBBwEVFcE6OAAA8xFwEFDHJxkzBwcAYB4CDgIq2nH8WVRn4FNCAAABQsBBQLXdRWUYUpPHa3E1AIBQRcBBQEVFhPl+5lZxAIBZCDgIqDC7TY7w1j87JhoDAMxCwEHA+SYaewg4AABzEHAQcMdXMybgAADMQcBBwPkW+3MzBwcAYA4CDgKO1YwBAGYj4CDgoiKPr4UDAIAZCDgIuLY5OKxmDAAwCwEHAccQFQDAbAQcBBwBBwBgNgIOAu74beIMUQEAzEHAQcBF0YMDADAZAQcBF33seVRHCDgAAJMQcBBw0Q5WMgYAmIuAg4BjkjEAwGwEHATc8YDDJGMAgDkIOAi4qAh6cAAA5iLgIOCOr2RMwAEAmIOAg4CLdrQ9i4ohKgCAOQg4CLi2OTj04AAAzELAQcBFR7QOUR12E3AAAOYg4CDg2lYyPuJpkddrWFwNACAUEXAQcG1DVJLU1EwvDgDA/wg4CLi228QlbhUHAJiDgIOAs9ttvpDDRGMAgBkIOLBE2zDVYVYzBgCYgIADS0TxPCoAgIlMDTg1NTXKzs5WbGysnE6n5s6dq4aGhk733717t2w2W4evl156ybdfR+/n5+eb2RT4GWvhAADMFG7mybOzs1VZWamCggJ5PB7dcsstmjdvnlatWtXh/snJyaqsrGy37YknntBDDz2kmTNnttv+zDPPKCsry/e70+n0e/0wT9vjGujBAQCYwbSAU1ZWpjVr1mjz5s2aMmWKJGnFihWaNWuWHn74YSUlJZ1wTFhYmFwuV7ttq1ev1rXXXqt+/fq12+50Ok/YF70HTxQHAJjJtCGqwsJCOZ1OX7iRpMzMTNntdhUVFZ3SOUpKSlRaWqq5c+ee8N78+fM1cOBATZ06VU8//bQMo/MF49xut+rr69u9YK1o5uAAAExkWg9OVVWVEhMT239YeLji4+NVVVV1Sud46qmnNGbMGE2bNq3d9gceeECXXHKJoqOjtXbtWt1xxx1qaGjQXXfd1eF5li5dqiVLlpxeQ2CKKIaoAAAm6nYPTl5eXqcTgdteO3fu7HFhR44c0apVqzrsvbn33nt14YUXauLEibrnnnv04x//WA899FCn51q8eLHq6up8r7179/a4PvRMtG8dHIaoAAD+1+0enEWLFmnOnDkn3SclJUUul0vV1dXttjc3N6umpuaU5s68/PLLamxs1E033dTlvmlpafrFL34ht9sth8NxwvsOh6PD7bBOtIMhKgCAebodcBISEpSQkNDlfunp6aqtrVVJSYkmT54sSVq3bp28Xq/S0tK6PP6pp57SFVdccUqfVVpaqv79+xNiehHm4AAAzGTaHJwxY8YoKytLOTk5evzxx+XxeLRgwQLNnj3bdwfV/v37lZGRoeeee05Tp071Hfvxxx/rvffe0xtvvHHCeV977TUdPHhQF1xwgfr06aOCggI9+OCD+uEPf2hWU2CC47eJM0QFAPA/U9fBWblypRYsWKCMjAzZ7XZdc801Wr58ue99j8ej8vJyNTY2tjvu6aef1tChQzVjxowTzhkREaHHHntMubm5MgxDI0eO1KOPPqqcnBwzmwI/a3sWFT04AAAz2IyT3V8dourr6xUXF6e6ujrFxsZaXc4ZKb94j/Je2a6M0Yl6as43rC4HANALdOf7m2dRwRLRDm4TBwCYh4ADS7TdJt7oIeAAAPyPgANL+O6icjPJGADgfwQcWCKK28QBACYi4MASbbeJH2GICgBgAgIOLMHTxAEAZiLgwBJtAafJ41WL94xbqQAAYDICDizRNkQlMUwFAPA/Ag4s0SfCLput9WeGqQAA/kbAgSVsNpvvcQ1HuJMKAOBnBBxY5vgDNwk4AAD/IuDAMtGshQMAMAkBB5bhVnEAgFkIOLAMqxkDAMxCwIFl2npwmGQMAPA3Ag4swyRjAIBZCDiwDHNwAABmIeDAMtxFBQAwCwEHlomKYIgKAGAOAg4sc3ySMUNUAAD/IuDAMtEOhqgAAOYg4MAy0ceeRdXI08QBAH5GwIFlfLeJuxmiAgD4FwEHlmElYwCAWQg4sIxvkjFDVAAAPyPgwDKsZAwAMAsBB5bhWVQAALMQcGCZtoBzmHVwAAB+RsCBZZhkDAAwCwEHlmmbg3O02asWr2FxNQCAUELAgWXahqgknigOAPAvAg4s4wi3y25r/ZmJxgAAfyLgwDI2m41bxQEApiDgwFJR3EkFADABAQeWYi0cAIAZCDiwFENUAAAzEHBgqWjWwgEAmICAA0sdf+Amc3AAAP5jWsCpqalRdna2YmNj5XQ6NXfuXDU0NJz0mKqqKt14441yuVzq27evJk2apL/85S89Pi+CV1TEsUnGbnpwAAD+Y1rAyc7O1o4dO1RQUKDXX39d7733nubNm3fSY2666SaVl5frb3/7m7Zv367vfve7uvbaa/X+++/36LwIXkwyBgCYwZSAU1ZWpjVr1ujJJ59UWlqapk+frhUrVig/P18HDhzo9LiNGzfqzjvv1NSpU5WSkqKf/exncjqdKikp6dF5EbyiHUwyBgD4nykBp7CwUE6nU1OmTPFty8zMlN1uV1FRUafHTZs2TS+88IJqamrk9XqVn5+vpqYmXXzxxT06r9vtVn19fbsXgkP0sSGqRubgAAD8yJSAU1VVpcTExHbbwsPDFR8fr6qqqk6Pe/HFF+XxeDRgwAA5HA7deuutWr16tUaOHNmj8y5dulRxcXG+V3Jycg9aB39iiAoAYIZuBZy8vDzZbLaTvnbu3Hnaxdx7772qra3Vm2++qS1btmjhwoW69tprtX379tM+pyQtXrxYdXV1vtfevXt7dD74T9SxdXCYZAwA8Kfw7uy8aNEizZkz56T7pKSkyOVyqbq6ut325uZm1dTUyOVydXhcRUWFfv/73+uDDz7QeeedJ0lKTU3VP//5Tz322GN6/PHHT+u8kuRwOORwOE6hhQg0bhMHAJihWwEnISFBCQkJXe6Xnp6u2tpalZSUaPLkyZKkdevWyev1Ki0trcNjGhsbJUl2e/tOpbCwMHm93tM+L4IbC/0BAMxgyhycMWPGKCsrSzk5OSouLtaGDRu0YMECzZ49W0lJSZKk/fv3a/To0SouLpYkjR49WiNHjtStt96q4uJiVVRU6JFHHlFBQYGuuuqqUz4vehce1QAAMINp6+CsXLlSo0ePVkZGhmbNmqXp06friSee8L3v8XhUXl7u67mJiIjQG2+8oYSEBF1++eUaP368nnvuOT377LOaNWvWKZ8XvQuTjAEAZrAZhmFYXUSg1dfXKy4uTnV1dYqNjbW6nDPapk++0OwnNikloa/WLbrY6nIAAEGsO9/fPIsKlqIHBwBgBgIOLMUcHACAGQg4sBQ9OAAAMxBwYKm2gHO0xStPi9fiagAAoYKAA0tFHQs4EsNUAAD/IeDAUpFhdoXZbZIYpgIA+A8BB5ay2WxfWc2YxzUAAPyDgAPL8bgGAIC/EXBgubZbxY94CDgAAP8g4MByURGtPTiH3QxRAQD8g4ADy7EWDgDA3wg4sFy0g9WMAQD+RcCB5aKPDVE1MgcHAOAnBBxY7vgQFXNwAAD+QcCB5dpWMz7spgcHAOAfBBxYzteDwxAVAMBPCDiwXNs6OKxkDADwFwIOLMdKxgAAfyPgwHKsgwMA8DcCDiwXdWyI6jABBwDgJwQcWI7bxAEA/kbAgeWYgwMA8DcCDizne5o4AQcA4CcEHFiOHhwAgL8RcGA530rGzMEBAPgJAQeW4zZxAIC/EXBgubY5OM1eQ0ebvRZXAwAIBQQcWK6tB0eiFwcA4B8EHFguIsyuiDCbJKnRwzwcAEDPEXAQFKIijk00dtODAwDoOQIOggJr4QAA/ImAg6AQ7WhbC4chKgBAzxFwEBR8i/156MEBAPQcAQdBITqCISoAgP8QcBAUfKsZuxmiAgD0HAEHQcG3mjFDVAAAPyDgICi03UXFAzcBAP5AwEFQ4IniAAB/Mi3g1NTUKDs7W7GxsXI6nZo7d64aGhpOekxVVZVuvPFGuVwu9e3bV5MmTdJf/vKXdvsMHz5cNput3WvZsmVmNQMBcvyBm8zBAQD0XLhZJ87OzlZlZaUKCgrk8Xh0yy23aN68eVq1alWnx9x0002qra3V3/72Nw0cOFCrVq3Stddeqy1btmjixIm+/R544AHl5OT4fo+JiTGrGQiQKHpwAAB+ZEoPTllZmdasWaMnn3xSaWlpmj59ulasWKH8/HwdOHCg0+M2btyoO++8U1OnTlVKSop+9rOfyel0qqSkpN1+MTExcrlcvlffvn3NaAYCiCEqAIA/mRJwCgsL5XQ6NWXKFN+2zMxM2e12FRUVdXrctGnT9MILL6impkZer1f5+flqamrSxRdf3G6/ZcuWacCAAZo4caIeeughNTeffFjD7Xarvr6+3QvB5fgkY4aoAAA9Z8oQVVVVlRITE9t/UHi44uPjVVVV1elxL774oq677joNGDBA4eHhio6O1urVqzVy5EjfPnfddZcmTZqk+Ph4bdy4UYsXL1ZlZaUeffTRTs+7dOlSLVmypOcNg2nowQEA+FO3enDy8vJOmOD79dfOnTtPu5h7771XtbW1evPNN7VlyxYtXLhQ1157rbZv3+7bZ+HChbr44os1fvx43XbbbXrkkUe0YsUKud3uTs+7ePFi1dXV+V579+497RphjuOTjAk4AICe61YPzqJFizRnzpyT7pOSkiKXy6Xq6up225ubm1VTUyOXy9XhcRUVFfr973+vDz74QOedd54kKTU1Vf/85z/12GOP6fHHH+/wuLS0NDU3N2v37t0aNWpUh/s4HA45HI4uWgcrRbEODgDAj7oVcBISEpSQkNDlfunp6aqtrVVJSYkmT54sSVq3bp28Xq/S0tI6PKaxsVGSZLe371QKCwuT1+vt9LNKS0tlt9tPGBJD73J8iIo5OACAnjNlkvGYMWOUlZWlnJwcFRcXa8OGDVqwYIFmz56tpKQkSdL+/fs1evRoFRcXS5JGjx6tkSNH6tZbb1VxcbEqKir0yCOPqKCgQFdddZWk1snLv/3tb7V161Z98sknWrlypXJzc3XDDTeof//+ZjQFAcIcHACAP5m2Ds7KlSu1YMECZWRkyG6365prrtHy5ct973s8HpWXl/t6biIiIvTGG28oLy9Pl19+uRoaGjRy5Eg9++yzmjVrlqTWoab8/Hzdf//9crvdGjFihHJzc7Vw4UKzmoEAabuLijk4AAB/sBmGYVhdRKDV19crLi5OdXV1io2NtbocSDpY36S0B99SmN2mj385UzabzeqSAABBpjvf3zyLCkGhbSXjFq+hoy2dz7kCAOBUEHAQFKIjwnw/N7oZpgIA9AwBB0EhPMyuyPDWP8dGDwEHANAzBBwEDZ4oDgDwFwIOgkbbMBW3igMAeoqAg6ARxVo4AAA/IeAgaPBEcQCAvxBwEDRYzRgA4C8EHAQNAg4AwF8IOAgaPK4BAOAvBBwEDSYZAwD8hYCDoHF8iIpJxgCAniHgIGgcv4uKHhwAQM8QcBA0mGQMAPAXAg6CBo9qAAD4CwEHQYNJxgAAfyHgIGgwRAUA8BcCDoIGj2oAAPgLAQdBgx4cAIC/EHAQNHyTjD0EHABAzxBwEDSiIlgHBwDgHwQcBA3fEJWbOTgAgJ4h4CBoRDuOBRxPiwzDsLgaAEBvRsBB0Gi7i8owJHez1+JqAAC9GQEHQSMqIsz3M/NwAAA9QcBB0Aiz2+QIb/2TZC0cAEBPEHAQVFgLBwDgDwQcBJXjqxkTcAAAp4+Ag6ByvAeHISoAwOkj4CCo+FYzpgcHANADBBwElSjm4AAA/ICAg6DCE8UBAP5AwEFQ4S4qAIA/EHAQVAg4AAB/IOAgqLQNUTHJGADQEwQcBBUmGQMA/CHc6gKAr+rnaP2T/Mu/9qlPhF03XHCWkpxRFlcFAOht6MFBUJkxdpCG9o9S3RGP/uedCn3z12/rjpUlKvrkCxmGYXV5AIBewrSAU1NTo+zsbMXGxsrpdGru3LlqaGg46TEVFRW6+uqrlZCQoNjYWF177bU6ePBgj8+L3uOcQTF690ff1h9vnKz0lAFq8Rp6Y3uVrntik2YtX68XNu9Rk4fhKwDAyZkWcLKzs7Vjxw4VFBTo9ddf13vvvad58+Z1uv/hw4c1Y8YM2Ww2rVu3Ths2bNDRo0d1+eWXy+v1nvZ50fuE2W269DyX/jzvAq25+5u6fuow9Ymwq6yyXvf8ZbsuWPqWlv19p/bXHrG6VABAkLIZJvT7l5WVaezYsdq8ebOmTJkiSVqzZo1mzZqlffv2KSkp6YRj1q5dq5kzZ+rLL79UbGysJKmurk79+/fX2rVrlZmZeVrn7Uh9fb3i4uJUV1fn+ywEt9rGo3pxy149u/FTX7Cx26QZY136TupgRYadWlaPCLNr2sgBcoSHmVkuAMAE3fn+NmWScWFhoZxOpy+ESFJmZqbsdruKiop09dVXn3CM2+2WzWaTw+HwbevTp4/sdrvWr1+vzMzM0zpv27ndbrfv9/r6en80EwHkjI7UvIvO1tzpKXqr7KD+tHG3NlZ8oTU7qrRmR1W3znX+kFi9fNs09Ykg5ABAqDIl4FRVVSkxMbH9B4WHKz4+XlVVHX8ZXXDBBerbt6/uuecePfjggzIMQ3l5eWppaVFlZeVpn1eSli5dqiVLlvSwVQgGYXabZpzn0ozzXCqvOqT/t2m3/n3g1APrRwcb9MH+ev381Q/06/9INbFSAICVuhVw8vLy9Ktf/eqk+5SVlZ1WIQkJCXrppZd0++23a/ny5bLb7br++us1adIk2e09myq0ePFiLVy40Pd7fX29kpOTe3ROWG+UK0b/fdW4bh2z/qPPddPTRXpxyz5NPqu/rvvGMJOqAwBYqVsBZ9GiRZozZ85J90lJSZHL5VJ1dXW77c3NzaqpqZHL5er02BkzZqiiokKff/65wsPD5XQ65XK5lJKSIkmnfV6Hw9Fu6AtnrunnDNSiGaP00D/Kde+rO3ReUpzOHxJndVkAAD/rVsBJSEhQQkJCl/ulp6ertrZWJSUlmjx5siRp3bp18nq9SktL6/L4gQMH+o6prq7WFVdc4ZfzApJ0+7fO1vt7vtSbZdW67fkSvX7ndDmjI60uCwDgR6bcJj5mzBhlZWUpJydHxcXF2rBhgxYsWKDZs2f77nTav3+/Ro8ereLiYt9xzzzzjDZt2qSKigo9//zz+t73vqfc3FyNGjXqlM8LdMVut+mR703QsPho7fvyiHJfKJXXyyKCABBKTFsHZ+XKlRo9erQyMjI0a9YsTZ8+XU888YTvfY/Ho/LycjU2Nvq2lZeX66qrrtKYMWP0wAMP6Kc//akefvjhbp0XOBVx0RH6ww2T5Ai36+3yz/T7tz+2uiQAgB+Zsg5OsGMdHLR5ccte/fjlbbLZpGdvmaqLzu16CBYAYI3ufH/zLCqc0a6dkqzrpybLMKQf5L/P6sgAECIIODjj3Xf5eRo3JE5fNnp0x/MlcjfzrCsA6O0IODjj9YkI0/9kT1JcVIS27qvTL17/t9UlAQB6iIADSEqOj9ZvZ0+QzSY9v2mPXvnXPqtLAgD0AAEHOObboxJ11yXnSJJ+snq7yip5ZhkA9FYEHOAr7so4Rxedm6Amj1e3P1+i+iaP1SUBAE4DAQf4ijC7Tb+7boKGOKO0+4tG5eaX6qODh9TCQoAA0KuwDg7r4KADW/fW6nuPF+poi1eS1DcyTOcNidP4IXEaNzRO44c6dVZ8tOx2m8WVAsCZozvf3wQcAg468VbZQf3hnQrtOFCvI54Tbx2P6ROucW2BZ4hT44fGKSHGIZtNCrPZZLfZCEAA4EcEnC4QcNAdLV5DFZ81aNu+Om3fV6tt++u040C9jjZ7T+l4u6116Mtmsx0LPq3Pw+oTEaaxg2M1IdmpCclOpSY7Fd+Xh34CQGcIOF0g4KCnPC1efXjwkLbvq9O2/XXavq9OO6vq5Wnp2T9Ow+KjfWFnQrJT5yXFqk9EmJ+qBoDejYDTBQIOzOBp8crd7JXXMOT1GvIarb0/hmGoxWj9vXW7oRavobojHm3fX6fSPbUq3VerTz47fMI5w+02jRkcq9TkOE1I7q8JyXFKGdiPoS8AZyQCThcIOAhGdY0ebdtfq9I9tdq6r1ale2v1ecPRE/aLcYRrfHKcUoce7+kZFNvHgooBILAIOF0g4KA3MAxD+2uPqHRva+jZtq9O2/fXdTjh2RXbxze0lZocp3FD4hTTJ8KCqgHAPAScLhBw0Fs1t3j14cEGbd1Xq617W3t5Pjx4SF9fpsdmk85J7KfUoU5NGNbayzNqUIzCw1j6CkDvRcDpAgEHoaTxaLM+2F+v0r1fauveOpXurdX+2iMn7BcVEaZxQ+J8gWdCslOD4/rIZmM+D4DegYDTBQIOQt1nh9ytQ1t7v1Tp3lpt21unQ+7mE/ZLjHG0hp1hTqUOdWrc0DjFMrQFIEgRcLpAwMGZxntsLZ/3jw1rle6pVXknj6BISeir1KGtCxemJjs1djC3qgMIDgScLhBwAOnI0RZ9cOD4bepb99Zq35cnDm2F220a5YppncA8NE7jhjgV0ydcNptkO7ZwoU221t/Vuu2rP0dHhhGQAPgFAacLBBygY180uLVtf5227m29a2vr3lp9cfjEW9W7w26Tzh0Uc/wur6FOnTuoHxOeAXQbAacLBBzg1LTdqt4Wdrbuq1VZ5SG5m1tkGJJxbJ+v/nwqD17vE2HXuCHt1/IZ2j+KCc8AToqA0wUCDmC+tuDzWUPrhOe2gNTZhOcBfSM1fmic4vs6Tvkz+kTYdf6QOE1IdurcQTEKY4VnIKQRcLpAwAGs4/Ua+uTzw+1CT1llz5/j1TcyTOOGxmnisP6akOzUxGSnElnhGQgpBJwuEHCA4NLkaVFZZb22769T49ETV2ruTG2j59h8oVod7uC4Ic6o1rBzbO0fV9zxwNM2HGbz/X78OJtsigizKb5vJMNmQBAh4HSBgAOElhavoY+rG/T+ni+Prf/Teht8T//tNrBf5LFb5p2+53/F9430T9EAuo2A0wUCDhD6GtzN2nbsoaXv72kdDqtv8kiSL/j4/uX3lX8LGsd+afYaHQak5PgojR/q1IRjawWdPyROfR3h5jUEgA8BpwsEHABdafK0aMeBet8Q2LZ9dfrk88Mn7Ge3SSMT+2lYfLR0bD2gr64NZD+2KFDbukB2m+QIt+s/Jidr6oj4gLcL6M0IOF0g4AA4HXVHPNq+r873sNNt++pUVd902uf7ftow5c0czeMxgFNEwOkCAQeAv1TXN2nrvjp93uA+th7Q8XWBZLQOeHm9xrF1glq376ys10sl+yS1Pg/sgSvPV9b5LusaAfQSBJwuEHAAWK3oky+0+JXtvmGvrPNcWnLleRrEre1Ap7rz/c1a6QBggbSUAXrjB9/Ugm+PVLjdpjU7qpT56LtaVbRH3lNZDhrASRFwAMAifSLC9MNLR+m1O6crdWicDjU16yert2v2/92kis8arC4P6NUIOABgsTGDY/XKHRfq3u+MVVREmIp31Wjm7/6p36/7SEebvVaXB/RKBBwACAJhdpvmTh+htbkX6aJzE3S02auH136oy1esV8mnX1pdHtDrMMmYScYAgoxhGHq19ICWvLZDXza2Lk44dUS85kwbrv8zdpAiwvhvU5yZuIuqCwQcAL3BFw1uLf37Tq1+f79ajk08dsX20Q0XDNPsqcM0sN+pP3kdCAUEnC4QcAD0JlV1TVpZ9Kn+XLxHnzcclSRFhtn1nfGDdfO04UpNdlpbIBAgBJwuEHAA9Ebu5ha9sb1Sf9r4qbburfVtn5Ds1JxpwzVznEuO8DDrCgRMFhQBp6amRnfeeadee+012e12XXPNNfrd736nfv36dXpMRUWFfvjDH2r9+vVyu93KysrSihUrNGjQIN8+w4cP16efftruuKVLlyovL++UayPgAOjtSvfW6rmNu/X6tkodbWm902pgP4eun5qsUa6YgNRgk631f21tv3/lPdvxvU5VYqxDYwfHqk8EIQ0dC4qAM3PmTFVWVuqPf/yjPB6PbrnlFn3jG9/QqlWrOtz/8OHDGj9+vFJTU7VkyRJJ0r333qsDBw5o06ZNsttbJ9UNHz5cc+fOVU5Oju/YmJgY9e3b95RrI+AACBWfHXIrv3iPni/6VAfr3VaX02MRYTaNGRyr1KFOTUh2KjXZqZSBfWW3n3pQQuiyPOCUlZVp7Nix2rx5s6ZMmSJJWrNmjWbNmqV9+/YpKSnphGPWrl2rmTNn6ssvv/QVXVdXp/79+2vt2rXKzMyU1Bpw7r77bt19992nXR8BB0Co8bR4tXbHQa1+f78ONXlM/zyjk1+Mr/zSnW8XQ9KnXxz2zTH6qpg+4e0Cz4RkpxJimGB9JurO93e4GQUUFhbK6XT6wo0kZWZmym63q6ioSFdfffUJx7jdbtlsNjkcx/9o+/TpI7vdrvXr1/sCjiQtW7ZMv/jFLzRs2DB9//vfV25ursLDO2+K2+2W2338v2zq6+t72kQACCoRYXZdNn6wLhs/2OpSTpthGNpfe0Sle1uf1l66t1bb99fpUFOz1n/8udZ//LlvX1dsH0VF9r6hLJtNumRUonL/z7nq6zDlKxjHmPL/blVVlRITE9t/UHi44uPjVVVV1eExF1xwgfr27at77rlHDz74oAzDUF5enlpaWlRZWenb76677tKkSZMUHx+vjRs3avHixaqsrNSjjz7aaT1Lly71DXsBAIKTzWbT0P7RGto/Wt8Z39rT72nx6sODh9qFno+qG1RV32Rxtafvk8926e8fVOm/rz5f3x6V2PUBOC3dGqLKy8vTr371q5PuU1ZWpldeeUXPPvusysvL272XmJioJUuW6Pbbb+/w2LVr1+r222/Xrl27ZLfbdf311+vf//63pk6dqj/84Q8dHvP000/r1ltvVUNDQ7ven6/qqAcnOTmZISoA6IUa3M366OAh39pAvcnBereW/r1M+748Ikm6ckKSfv6dsRrAmkanxLQhqkWLFmnOnDkn3SclJUUul0vV1dXttjc3N6umpkYul6vTY2fMmKGKigp9/vnnCg8Pl9PplMvlUkpKSqfHpKWlqbm5Wbt379aoUaM63MfhcHQafgAAvUs/R7gmDutvdRmn7dujE/Sbgg/11PpderX0gN778DPd+52xunriENlsTKb2l24FnISEBCUkJHS5X3p6umpra1VSUqLJkydLktatWyev16u0tLQujx84cKDvmOrqal1xxRWd7ltaWiq73X7CkBgAAMEoOjJcP71srC5PTdI9f9mussp6LXxxq1a/v18PXj1OyfHRVpcYEkx5oMmYMWOUlZWlnJwcFRcXa8OGDVqwYIFmz57tu4Nq//79Gj16tIqLi33HPfPMM9q0aZMqKir0/PPP63vf+55yc3N9PTOFhYX67W9/q61bt+qTTz7RypUrlZubqxtuuEH9+/feNA8AOPOMH+rU3xZcqB9njVJkuF3//OhzzfjNe3ryn5+ouYWnyPeUaVO4V65cqQULFigjI8O30N/y5ct973s8HpWXl6uxsdG3rby8XIsXL1ZNTY2GDx+un/70p8rNzfW973A4lJ+fr/vvv19ut1sjRoxQbm6uFi5caFYzAAAwTUSYXXdcPFIzzx+sxa9s06ZPavTf/1umV0sPaNk143ReUpzVJfZaPKqBScYAgCBgGIZe3LJXv/zfMtU3NSvMblN22jANcUZZXZokKTXZqQtSBlhag+UL/QU7Ag4AIFhVH2rSkr/9W/+7vbLrnQPs7sxz9IOMcyybDE3A6QIBBwAQ7N4qO6h/7KhSMEzHqTvi0ZtlByVJV08comXXjLPkwa4EnC4QcAAA6J4/F+/Rz/76gVq8hqYOj9cfb5ys/n0jA1pDd76/TbmLCgAAhJbrpw7Ts7dMVYwjXMW7a3T1/2zQJ581WF1Wpwg4AADglEw/Z6D+csc0DXFGafcXjfruHzaq6JMvrC6rQwQcAABwys4dFKO/zr9QE5Kdqm306IanivTKv/ZZXdYJCDgAAKBbEmIcyp93gWaNc8nTYmjhi1v1aMGHCqZpvQQcAADQbX0iwvT76yfp9ovPliQtf+sj/SC/VE2eFosra0XAAQAAp8Vut+merNH61TXjFG636W9bD+iGJ4v0RYPb6tIIOAAAoGeu+8YwPfufUxXTJ1xbPv1SV//PRlVYfIcVAQcAAPTYhSMHavUd05QcH6U9NY368cvbLJ2TQ8ABAAB+MTIxRqvvuFCZYwbpN9dOsOyRDpKJTxMHAABnnoH9HHry5ilWl0EPDgAACD0EHAAAEHIIOAAAIOQQcAAAQMgh4AAAgJBDwAEAACGHgAMAAEIOAQcAAIQcAg4AAAg5BBwAABByCDgAACDkEHAAAEDIIeAAAICQc0Y+TdwwDElSfX29xZUAAIBT1fa93fY9fjJnZMA5dOiQJCk5OdniSgAAQHcdOnRIcXFxJ93HZpxKDAoxXq9XBw4cUExMjGw2m1/PXV9fr+TkZO3du1exsbF+PXewOBPaKNHOUEM7Q8eZ0EaJdnbEMAwdOnRISUlJsttPPsvmjOzBsdvtGjp0qKmfERsbG9J/kNKZ0UaJdoYa2hk6zoQ2SrTz67rquWnDJGMAABByCDgAACDkEHD8zOFw6L777pPD4bC6FNOcCW2UaGeooZ2h40xoo0Q7e+qMnGQMAABCGz04AAAg5BBwAABAyCHgAACAkEPAAQAAIYeA40ePPfaYhg8frj59+igtLU3FxcVWl+RX999/v2w2W7vX6NGjrS6rx9577z1dfvnlSkpKks1m01//+td27xuGoZ///OcaPHiwoqKilJmZqY8++siaYnugq3bOmTPnhOublZVlTbGnaenSpfrGN76hmJgYJSYm6qqrrlJ5eXm7fZqamjR//nwNGDBA/fr10zXXXKODBw9aVPHpOZV2XnzxxSdcz9tuu82iik/PH/7wB40fP963AFx6err+/ve/+94PhWspdd3OULiWX7ds2TLZbDbdfffdvm3+vp4EHD954YUXtHDhQt13333617/+pdTUVF166aWqrq62ujS/Ou+881RZWel7rV+/3uqSeuzw4cNKTU3VY4891uH7v/71r7V8+XI9/vjjKioqUt++fXXppZeqqakpwJX2TFftlKSsrKx21/fPf/5zACvsuXfffVfz58/Xpk2bVFBQII/HoxkzZujw4cO+fXJzc/Xaa6/ppZde0rvvvqsDBw7ou9/9roVVd9+ptFOScnJy2l3PX//61xZVfHqGDh2qZcuWqaSkRFu2bNEll1yiK6+8Ujt27JAUGtdS6rqdUu+/ll+1efNm/fGPf9T48ePbbff79TTgF1OnTjXmz5/v+72lpcVISkoyli5damFV/nXfffcZqampVpdhKknG6tWrfb97vV7D5XIZDz30kG9bbW2t4XA4jD//+c8WVOgfX2+nYRjGzTffbFx55ZWW1GOW6upqQ5Lx7rvvGobReu0iIiKMl156ybdPWVmZIckoLCy0qswe+3o7DcMwvvWtbxk/+MEPrCvKJP379zeefPLJkL2WbdraaRihdS0PHTpknHPOOUZBQUG7dplxPenB8YOjR4+qpKREmZmZvm12u12ZmZkqLCy0sDL/++ijj5SUlKSUlBRlZ2drz549Vpdkql27dqmqqqrdtY2Li1NaWlrIXVtJeuedd5SYmKhRo0bp9ttv1xdffGF1ST1SV1cnSYqPj5cklZSUyOPxtLueo0eP1rBhw3r19fx6O9usXLlSAwcO1Pnnn6/FixersbHRivL8oqWlRfn5+Tp8+LDS09ND9lp+vZ1tQuVazp8/X5dddlm76yaZ88/mGfmwTX/7/PPP1dLSokGDBrXbPmjQIO3cudOiqvwvLS1Nf/rTnzRq1ChVVlZqyZIl+uY3v6kPPvhAMTExVpdniqqqKknq8Nq2vRcqsrKy9N3vflcjRoxQRUWFfvKTn2jmzJkqLCxUWFiY1eV1m9fr1d13360LL7xQ559/vqTW6xkZGSmn09lu3958PTtqpyR9//vf11lnnaWkpCRt27ZN99xzj8rLy/XKK69YWG33bd++Xenp6WpqalK/fv20evVqjR07VqWlpSF1LTtrpxQ61zI/P1//+te/tHnz5hPeM+OfTQIOTtnMmTN9P48fP15paWk666yz9OKLL2ru3LkWVgZ/mD17tu/ncePGafz48Tr77LP1zjvvKCMjw8LKTs/8+fP1wQcfhMQ8sZPprJ3z5s3z/Txu3DgNHjxYGRkZqqio0Nlnnx3oMk/bqFGjVFpaqrq6Or388su6+eab9e6771pdlt911s6xY8eGxLXcu3evfvCDH6igoEB9+vQJyGcyROUHAwcOVFhY2AmzvQ8ePCiXy2VRVeZzOp0699xz9fHHH1tdimnart+Zdm0lKSUlRQMHDuyV13fBggV6/fXX9fbbb2vo0KG+7S6XS0ePHlVtbW27/Xvr9eysnR1JS0uTpF53PSMjIzVy5EhNnjxZS5cuVWpqqn73u9+F3LXsrJ0d6Y3XsqSkRNXV1Zo0aZLCw8MVHh6ud999V8uXL1d4eLgGDRrk9+tJwPGDyMhITZ48WW+99ZZvm9fr1VtvvdVuDDXUNDQ0qKKiQoMHD7a6FNOMGDFCLper3bWtr69XUVFRSF9bSdq3b5+++OKLXnV9DcPQggULtHr1aq1bt04jRoxo9/7kyZMVERHR7nqWl5drz549vep6dtXOjpSWlkpSr7qeHfF6vXK73SFzLTvT1s6O9MZrmZGRoe3bt6u0tNT3mjJlirKzs30/+/169nxONAzDMPLz8w2Hw2H86U9/Mv79738b8+bNM5xOp1FVVWV1aX6zaNEi45133jF27dplbNiwwcjMzDQGDhxoVFdXW11ajxw6dMh4//33jffff9+QZDz66KPG+++/b3z66aeGYRjGsmXLDKfTabz66qvGtm3bjCuvvNIYMWKEceTIEYsr756TtfPQoUPGD3/4Q6OwsNDYtWuX8eabbxqTJk0yzjnnHKOpqcnq0k/Z7bffbsTFxRnvvPOOUVlZ6Xs1Njb69rntttuMYcOGGevWrTO2bNlipKenG+np6RZW3X1dtfPjjz82HnjgAWPLli3Grl27jFdffdVISUkxLrroIosr7568vDzj3XffNXbt2mVs27bNyMvLM2w2m7F27VrDMELjWhrGydsZKteyI1+/O8zf15OA40crVqwwhg0bZkRGRhpTp041Nm3aZHVJfnXdddcZgwcPNiIjI40hQ4YY1113nfHxxx9bXVaPvf3224akE14333yzYRitt4rfe++9xqBBgwyHw2FkZGQY5eXl1hZ9Gk7WzsbGRmPGjBlGQkKCERERYZx11llGTk5OrwvoHbVPkvHMM8/49jly5Ihxxx13GP379zeio6ONq6++2qisrLSu6NPQVTv37NljXHTRRUZ8fLzhcDiMkSNHGj/60Y+Muro6awvvpv/8z/80zjrrLCMyMtJISEgwMjIyfOHGMELjWhrGydsZKteyI18POP6+njbDMIzT6/sBAAAITszBAQAAIYeAAwAAQg4BBwAAhBwCDgAACDkEHAAAEHIIOAAAIOQQcAAAQMgh4AAAgJBDwAEAACGHgAMAAEIOAQcAAIQcAg4AAAg5/x/aMV/5g9ZfigAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp['accuracy'].sort_values(ascending=False).reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off with parallelization, is that we will not optimize the search after each evaluation of f(x), instead after, in this case 4, evaluations of f(x). Thus, we may need to perform more evaluations to find the optima. But, because we do it in parallel, overall, we reduce wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>0.855144</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.967343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.068018</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077730</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.079010</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063095</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.034960</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039472</td>\n",
       "      <td>0.036322</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.942242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.934723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>0.940702</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.934666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "      <td>0.053117</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.932217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071915</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.932198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "      <td>0.078040</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.927166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019436</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.927109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078425</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.924660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.922135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.919648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.242187</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.914635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.894433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>deviance</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "13           120          5           0.076970       0.855144     deviance   \n",
       "25           120          1           0.100000       0.999000  exponential   \n",
       "22           120          5           0.100000       0.999000  exponential   \n",
       "24           120          1           0.100000       0.999000  exponential   \n",
       "27           120          1           0.100000       0.999000  exponential   \n",
       "38            65          1           0.068018       0.999000     deviance   \n",
       "16           120          1           0.077730       0.001000     deviance   \n",
       "20           120          3           0.078946       0.999000     deviance   \n",
       "21           120          1           0.079010       0.999000     deviance   \n",
       "32           120          5           0.035597       0.999000     deviance   \n",
       "36            46          1           0.055556       0.001000  exponential   \n",
       "39            48          1           0.063095       0.999000     deviance   \n",
       "17           120          1           0.076076       0.001000     deviance   \n",
       "18           120          1           0.076302       0.034960     deviance   \n",
       "37            57          1           0.100000       0.001000     deviance   \n",
       "34           120          2           0.039472       0.036322  exponential   \n",
       "7             42          4           0.089523       0.206446     deviance   \n",
       "23           120          2           0.017801       0.940702  exponential   \n",
       "33           120          4           0.053117       0.001000  exponential   \n",
       "12            25          3           0.071915       0.092214     deviance   \n",
       "4             38          5           0.053127       0.062772     deviance   \n",
       "28            64          1           0.025758       0.001000     deviance   \n",
       "19           114          5           0.078040       0.001000     deviance   \n",
       "30            74          1           0.019436       0.999000  exponential   \n",
       "14            10          2           0.078425       0.001189     deviance   \n",
       "10           107          5           0.016812       0.008609  exponential   \n",
       "29            71          5           0.018167       0.001000     deviance   \n",
       "0             68          4           0.007381       0.087048  exponential   \n",
       "26            10          1           0.100000       0.001000  exponential   \n",
       "11            10          2           0.051934       0.242187  exponential   \n",
       "15            10          4           0.081216       0.999000     deviance   \n",
       "35            89          1           0.007794       0.002690  exponential   \n",
       "31           120          5           0.004969       0.999000  exponential   \n",
       "8             68          1           0.001064       0.003752     deviance   \n",
       "6             56          1           0.000678       0.927447  exponential   \n",
       "5             28          4           0.005446       0.002259     deviance   \n",
       "3             42          4           0.000960       0.679353  exponential   \n",
       "2             17          3           0.000222       0.173717  exponential   \n",
       "1            118          2           0.000101       0.057260     deviance   \n",
       "9             48          4           0.001682       0.278868     deviance   \n",
       "\n",
       "    accuracy  \n",
       "13 -0.967343  \n",
       "25 -0.957280  \n",
       "22 -0.957280  \n",
       "24 -0.957280  \n",
       "27 -0.957280  \n",
       "38 -0.949761  \n",
       "16 -0.949761  \n",
       "20 -0.949761  \n",
       "21 -0.949761  \n",
       "32 -0.949761  \n",
       "36 -0.947254  \n",
       "39 -0.947254  \n",
       "17 -0.947236  \n",
       "18 -0.947236  \n",
       "37 -0.947236  \n",
       "34 -0.942242  \n",
       "7  -0.934723  \n",
       "23 -0.934666  \n",
       "33 -0.932217  \n",
       "12 -0.932198  \n",
       "4  -0.929654  \n",
       "28 -0.929654  \n",
       "19 -0.927166  \n",
       "30 -0.927109  \n",
       "14 -0.924660  \n",
       "10 -0.922135  \n",
       "29 -0.919648  \n",
       "0  -0.917141  \n",
       "26 -0.917027  \n",
       "11 -0.914635  \n",
       "15 -0.899465  \n",
       "35 -0.899465  \n",
       "31 -0.894433  \n",
       "8  -0.625636  \n",
       "6  -0.625636  \n",
       "5  -0.625636  \n",
       "3  -0.625636  \n",
       "2  -0.625636  \n",
       "1  -0.625636  \n",
       "9  -0.625636  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('hyperparameter-optimization': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2b5166dc44e1c447f633fd495edf8ade93e6e9a81bfb1e7ede5f55ce3b3ba8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
