{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will use **Bayesian Optimization** to select the best **hyperparameters** for a Gradient Boosting Classifier, from the xgb package. We optimize over the same hyperparameter space as in the notebook of Hyperopt in **section 5**, for comparison.\n",
    "\n",
    "### Hyperparameter Tunning Procedure\n",
    "\n",
    "To tune the hyper-parameters of our model we need to:\n",
    "\n",
    "- define a model\n",
    "- decide which parameters to optimize\n",
    "- define the objective function we want to minimize.\n",
    "\n",
    "\n",
    "### NOTE\n",
    "\n",
    "Scikit-Optimize will always **minimize** the objective function, so if we want to maximize a function, for example the roc-auc, we need to **negate** the metric. Thus, instead of maximizing the roc-auc, we minimize the -roc-auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(200, 2500, name='n_estimators'),\n",
    "    Integer(1, 10, name='max_depth'),\n",
    "    Real(0.01, 0.99, name='learning_rate'),\n",
    "    Categorical(['gbtree', 'dart'], name='booster'),\n",
    "    Real(0.01, 10, name='gamma'),\n",
    "    Real(0.50, 0.90, name='subsample'),\n",
    "    Real(0.50, 0.90, name='colsample_bytree'),\n",
    "    Real(0.50, 0.90, name='colsample_bylevel'),\n",
    "    Real(0.50, 0.90, name='colsample_bynode'),\n",
    "    Integer(1, 50, name='reg_lambda'),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = xgb.XGBClassifier(random_state=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement of Scikit-Optimize.\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=4,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "We are now ready for sequential model-based optimization. Here we use Gaussian process-based Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:13:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:13:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:13:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gp_minimize performs by default GP Optimization \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# using a Marten Kernel\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m gp_ \u001b[38;5;241m=\u001b[39m gp_minimize(\n\u001b[1;32m      5\u001b[0m     objective, \u001b[38;5;66;03m# the objective function to minimize\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     param_grid, \u001b[38;5;66;03m# the hyperparameter space\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     n_initial_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m# the number of points to evaluate f(x) to start of\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     acq_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# the acquisition function\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     n_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, \u001b[38;5;66;03m# the number of subsequent evaluations of f(x)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/skopt/optimizer/gp.py:259\u001b[0m, in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m base_estimator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     base_estimator \u001b[39m=\u001b[39m cook_estimator(\n\u001b[1;32m    256\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGP\u001b[39m\u001b[39m\"\u001b[39m, space\u001b[39m=\u001b[39mspace, random_state\u001b[39m=\u001b[39mrng\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax),\n\u001b[1;32m    257\u001b[0m         noise\u001b[39m=\u001b[39mnoise)\n\u001b[0;32m--> 259\u001b[0m \u001b[39mreturn\u001b[39;00m base_minimize(\n\u001b[1;32m    260\u001b[0m     func, space, base_estimator\u001b[39m=\u001b[39;49mbase_estimator,\n\u001b[1;32m    261\u001b[0m     acq_func\u001b[39m=\u001b[39;49macq_func,\n\u001b[1;32m    262\u001b[0m     xi\u001b[39m=\u001b[39;49mxi, kappa\u001b[39m=\u001b[39;49mkappa, acq_optimizer\u001b[39m=\u001b[39;49macq_optimizer, n_calls\u001b[39m=\u001b[39;49mn_calls,\n\u001b[1;32m    263\u001b[0m     n_points\u001b[39m=\u001b[39;49mn_points, n_random_starts\u001b[39m=\u001b[39;49mn_random_starts,\n\u001b[1;32m    264\u001b[0m     n_initial_points\u001b[39m=\u001b[39;49mn_initial_points,\n\u001b[1;32m    265\u001b[0m     initial_point_generator\u001b[39m=\u001b[39;49minitial_point_generator,\n\u001b[1;32m    266\u001b[0m     n_restarts_optimizer\u001b[39m=\u001b[39;49mn_restarts_optimizer,\n\u001b[1;32m    267\u001b[0m     x0\u001b[39m=\u001b[39;49mx0, y0\u001b[39m=\u001b[39;49my0, random_state\u001b[39m=\u001b[39;49mrng, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    268\u001b[0m     callback\u001b[39m=\u001b[39;49mcallback, n_jobs\u001b[39m=\u001b[39;49mn_jobs, model_queue_size\u001b[39m=\u001b[39;49mmodel_queue_size)\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/skopt/optimizer/base.py:301\u001b[0m, in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_calls):\n\u001b[1;32m    300\u001b[0m     next_x \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mask()\n\u001b[0;32m--> 301\u001b[0m     next_y \u001b[39m=\u001b[39m func(next_x)\n\u001b[1;32m    302\u001b[0m     result \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mtell(next_x, next_y)\n\u001b[1;32m    303\u001b[0m     result\u001b[39m.\u001b[39mspecs \u001b[39m=\u001b[39m specs\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/skopt/utils.py:803\u001b[0m, in \u001b[0;36muse_named_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    800\u001b[0m arg_dict \u001b[39m=\u001b[39m {dim\u001b[39m.\u001b[39mname: value \u001b[39mfor\u001b[39;00m dim, value \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(dimensions, x)}\n\u001b[1;32m    802\u001b[0m \u001b[39m# Call the wrapped objective function with the named arguments.\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m objective_value \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marg_dict)\n\u001b[1;32m    805\u001b[0m \u001b[39mreturn\u001b[39;00m objective_value\n",
      "Cell \u001b[0;32mIn [7], line 14\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(**params)\u001b[0m\n\u001b[1;32m     10\u001b[0m gbm\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# optimization function (hyperparam response function)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# negate because we need to minimize\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mvalue\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPass \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as keyword args. From version 0.25 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpassing these as positional arguments will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mresult in an error\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:401\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    399\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 401\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(estimator\u001b[39m=\u001b[39;49mestimator, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    402\u001b[0m                             scoring\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m'\u001b[39;49m: scorer}, cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    403\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    404\u001b[0m                             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    405\u001b[0m                             pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    406\u001b[0m                             error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPass \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as keyword args. From version 0.25 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpassing these as positional arguments will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mresult in an error\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:242\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    241\u001b[0m                     pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 242\u001b[0m scores \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    243\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m         clone(estimator), X, y, scorers, train, test, verbose, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    245\u001b[0m         fit_params, return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    246\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    247\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    248\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    250\u001b[0m zipped_scores \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mscores))\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/parallel.py:1054\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1054\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1055\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/parallel.py:933\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 933\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    934\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gp_minimize performs by default GP Optimization \n",
    "# using a Marten Kernel\n",
    "\n",
    "gp_ = gp_minimize(\n",
    "    objective, # the objective function to minimize\n",
    "    param_grid, # the hyperparameter space\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    n_calls=50, # the number of subsequent evaluations of f(x)\n",
    "    random_state=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best score=-0.9749'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function value at the minimum.\n",
    "# note that it is the negative of the accuracy\n",
    "\n",
    "\"Best score=%.4f\" % gp_.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2500,\n",
       " 7,\n",
       " 0.8985683883566217,\n",
       " 'dart',\n",
       " 0.01,\n",
       " 0.6149472077372051,\n",
       " 0.5199079632032467,\n",
       " 0.5,\n",
       " 0.5672549702813826,\n",
       " 11]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "=========================\n",
      "- n_estimators = 2500\n",
      "- max_depth = 7\n",
      "- learning_rate = 0.898568\n",
      "- booster = dart\n",
      "- gamma = 0.010000\n",
      "= subsample = 0.614947\n",
      "- colsample_bytree = 0.519908\n",
      "- colsample_bylevel = 0.500000\n",
      "- colsample_bynode' = 0.567255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Best parameters:\n",
    "=========================\n",
    "- n_estimators = %d\n",
    "- max_depth = %d\n",
    "- learning_rate = %.6f\n",
    "- booster = %s\n",
    "- gamma = %.6f\n",
    "= subsample = %.6f\n",
    "- colsample_bytree = %.6f\n",
    "- colsample_bylevel = %.6f\n",
    "- colsample_bynode' = %.6f\n",
    "\"\"\" % (gp_.x[0],\n",
    "       gp_.x[1],\n",
    "       gp_.x[2],\n",
    "       gp_.x[3],\n",
    "       gp_.x[4],\n",
    "       gp_.x[5],\n",
    "       gp_.x[6],\n",
    "       gp_.x[7],\n",
    "       gp_.x[8],\n",
    "      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence of the search\n",
    "\n",
    "[plot_convergence](https://scikit-optimize.github.io/stable/modules/generated/skopt.plots.plot_convergence.html#skopt.plots.plot_convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(gp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('hyperparameter-optimization': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2b5166dc44e1c447f633fd495edf8ade93e6e9a81bfb1e7ede5f55ce3b3ba8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
