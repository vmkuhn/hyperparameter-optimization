{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search with Hyperopt\n",
    "\n",
    "In this notebook, we will perform **Randomized Search** to select the best **hyperparameters** for a Gradient Boosting Classifier, using the open source Python package [Hyperopt](http://hyperopt.github.io/hyperopt/).\n",
    "\n",
    "The randomized search is performed with the class **rand**.\n",
    "\n",
    "I find the documentation for Hyperopt quite unintuitive, so it helps to refer to the [original article](https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008/pdf) to understand the different parameters and classes.\n",
    "\n",
    "To step out of Scikit-learn, we will optimise the parameters of a Gradient Boosting Machine of the [xgboost package](https://xgboost.readthedocs.io/en/latest/python/python_intro.html).\n",
    "\n",
    "\n",
    "### Procedure\n",
    "\n",
    "To tune the hyper-parameters of our model we need to:\n",
    "\n",
    "- define a model\n",
    "- define the hyperparameter space\n",
    "- define the objective function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import hp, rand, fmin, Trials\n",
    "\n",
    "# hp: define the hyperparameter space\n",
    "# rand: random search\n",
    "# fmin: optimization function\n",
    "# Trials: to evaluate the different searched hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "- [Hyperopt search space](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)\n",
    "\n",
    "- [xgb.XGBClassifier hyperparameters](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)\n",
    "\n",
    "- [xgb general parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 200, 2500, 100),\n",
    "    'max_depth': hp.uniform('max_depth', 1, 10),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.99),\n",
    "    'booster': hp.choice('booster', ['gbtree', 'dart']),\n",
    "    'gamma': hp.quniform('gamma', 0.01, 10, 0.1),\n",
    "    'subsample': hp.uniform('subsample', 0.50, 0.90),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the objective function takes the hyperparameter space\n",
    "# as input\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    # we need a dictionary to indicate which value from the space\n",
    "    # to attribute to each value of the hyperparameter in the xgb\n",
    "    params_dict = {\n",
    "        'n_estimators': int(params['n_estimators']), # important int, as it takes integers only\n",
    "        'max_depth': int(params['max_depth']), # important int, as it takes integers only\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'booster': params['booster'],\n",
    "        'gamma': params['gamma'],\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'colsample_bylevel': params['colsample_bylevel'],\n",
    "        'colsample_bynode': params['colsample_bynode'],\n",
    "        'random_state': 1000,\n",
    "    }\n",
    "\n",
    "    # with ** we pass the items in the dictionary as parameters\n",
    "    # to the xgb\n",
    "    gbm = xgb.XGBClassifier(**params_dict)\n",
    "\n",
    "    # train with cv\n",
    "    score = cross_val_score(gbm, X_train, y_train,\n",
    "                            scoring='accuracy', cv=5, n_jobs=-1).mean()\n",
    "\n",
    "    # to minimize, we negate the score\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "[fmin](http://hyperopt.github.io/hyperopt/getting-started/minimizing_functions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/kuhn/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:12:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:12:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "  0%|          | 0/50 [02:32<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# fmin performs the minimization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# rand.suggest samples the parameters at random\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# i.e., performs the random search\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m search \u001b[38;5;241m=\u001b[39m fmin(\n\u001b[1;32m      6\u001b[0m     fn\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m      7\u001b[0m     space\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m      8\u001b[0m     max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#rstate=42,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     algo\u001b[38;5;241m=\u001b[39mrand\u001b[38;5;241m.\u001b[39msuggest,  \u001b[38;5;66;03m# randomized search\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/hyperopt/fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    550\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    552\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/hyperopt/fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    357\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/hyperopt/fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    289\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/hyperopt/fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    171\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/hyperopt/base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    903\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    904\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    905\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    906\u001b[0m     )\n\u001b[0;32m--> 907\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    910\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn [13], line 26\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     23\u001b[0m gbm \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams_dict)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# train with cv\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgbm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# to minimize, we negate the score\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mscore\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPass \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as keyword args. From version 0.25 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpassing these as positional arguments will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mresult in an error\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:401\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    399\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 401\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(estimator\u001b[39m=\u001b[39;49mestimator, X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    402\u001b[0m                             scoring\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m'\u001b[39;49m: scorer}, cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    403\u001b[0m                             n_jobs\u001b[39m=\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    404\u001b[0m                             fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    405\u001b[0m                             pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    406\u001b[0m                             error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m'\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/utils/validation.py:72\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mPass \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m as keyword args. From version 0.25 \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mpassing these as positional arguments will \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mresult in an error\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(args_msg)),\n\u001b[1;32m     70\u001b[0m                   \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[1;32m     71\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({k: arg \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)})\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:242\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    241\u001b[0m                     pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 242\u001b[0m scores \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    243\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    244\u001b[0m         clone(estimator), X, y, scorers, train, test, verbose, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    245\u001b[0m         fit_params, return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    246\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    247\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score)\n\u001b[1;32m    248\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    250\u001b[0m zipped_scores \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mscores))\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/parallel.py:1054\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1054\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1055\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/parallel.py:933\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 933\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    934\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/Github_local/hyperparameter-optimization/.venv/hyperparameter-optimization/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m/usr/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fmin performs the minimization\n",
    "# rand.suggest samples the parameters at random\n",
    "# i.e., performs the random search\n",
    "\n",
    "search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=50,\n",
    "    #rstate=42,\n",
    "    algo=rand.suggest,  # randomized search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fmin returns a dictionary with the best parameters\n",
    "\n",
    "type(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 0,\n",
       " 'colsample_bylevel': 0.6968081788963586,\n",
       " 'colsample_bynode': 0.5468589490873155,\n",
       " 'colsample_bytree': 0.8873330143353462,\n",
       " 'gamma': 3.1,\n",
       " 'learning_rate': 0.884227960632757,\n",
       " 'max_depth': 2.6776077107023877,\n",
       " 'n_estimators': 900.0,\n",
       " 'reg_lambda': 13.43768203715936,\n",
       " 'subsample': 0.5584894776771878}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another dictionary to pass the search items as parameters\n",
    "# to a new xgb\n",
    "\n",
    "best_hp_dict = {\n",
    "        'n_estimators': int(search['n_estimators']), # important int, as it takes integers only\n",
    "        'max_depth': int(search['max_depth']), # important int, as it takes integers only\n",
    "        'learning_rate': search['learning_rate'],\n",
    "        'booster': 'gbtree',\n",
    "        'gamma': search['gamma'],\n",
    "        'subsample': search['subsample'],\n",
    "        'colsample_bytree': search['colsample_bytree'],\n",
    "        'colsample_bylevel': search['colsample_bylevel'],\n",
    "        'colsample_bynode': search['colsample_bynode'],\n",
    "        'random_state': 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=0.6968081788963586,\n",
       "              colsample_bynode=0.5468589490873155,\n",
       "              colsample_bytree=0.8873330143353462, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=3.1, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.884227960632757,\n",
       "              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=2,\n",
       "              max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=900, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=1000,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=0.6968081788963586,\n",
       "              colsample_bynode=0.5468589490873155,\n",
       "              colsample_bytree=0.8873330143353462, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=3.1, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.884227960632757,\n",
       "              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=2,\n",
       "              max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=900, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=1000,\n",
       "              reg_alpha=0, reg_lambda=1, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=0.6968081788963586,\n",
       "              colsample_bynode=0.5468589490873155,\n",
       "              colsample_bytree=0.8873330143353462, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=3.1, gpu_id=-1,\n",
       "              grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.884227960632757,\n",
       "              max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=2,\n",
       "              max_leaves=0, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=900, n_jobs=0,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=1000,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after the search we can train the model with the\n",
    "# best parameters manually\n",
    "\n",
    "gbm_final = xgb.XGBClassifier(**best_hp_dict)\n",
    "\n",
    "gbm_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.992462311557789\n",
      "Test accuracy:  0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "X_train_preds = gbm_final.predict(X_train)\n",
    "X_test_preds = gbm_final.predict(X_test)\n",
    "\n",
    "print('Train accuracy: ', accuracy_score(y_train, X_train_preds))\n",
    "print('Test accuracy: ', accuracy_score(y_test, X_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the search\n",
    "\n",
    "We can use Trials if we want to look into the search, and the performance values encountered during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆ| 50/50 [1:02:33<00:00, 75.06s/trial, best loss: -0.96740506\n"
     ]
    }
   ],
   "source": [
    "second_search = fmin(\n",
    "    fn=objective,\n",
    "    space=param_grid,\n",
    "    max_evals=50,\n",
    "    rstate=np.random.default_rng(42),\n",
    "    algo=rand.suggest,  # randomized search\n",
    "    trials = trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 0,\n",
       " 'colsample_bylevel': 0.6968081788963586,\n",
       " 'colsample_bynode': 0.5468589490873155,\n",
       " 'colsample_bytree': 0.8873330143353462,\n",
       " 'gamma': 3.1,\n",
       " 'learning_rate': 0.884227960632757,\n",
       " 'max_depth': 2.6776077107023877,\n",
       " 'n_estimators': 900.0,\n",
       " 'reg_lambda': 13.43768203715936,\n",
       " 'subsample': 0.5584894776771878}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "\n",
    "second_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': 0,\n",
       " 'colsample_bylevel': 0.6968081788963586,\n",
       " 'colsample_bynode': 0.5468589490873155,\n",
       " 'colsample_bytree': 0.8873330143353462,\n",
       " 'gamma': 3.1,\n",
       " 'learning_rate': 0.884227960632757,\n",
       " 'max_depth': 2.6776077107023877,\n",
       " 'n_estimators': 900.0,\n",
       " 'reg_lambda': 13.43768203715936,\n",
       " 'subsample': 0.5584894776771878}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the best hyperparameters can also be found in\n",
    "# trials\n",
    "\n",
    "trials.argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.737251</td>\n",
       "      <td>0.658760</td>\n",
       "      <td>0.850624</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.762499</td>\n",
       "      <td>6.865682</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>12.782261</td>\n",
       "      <td>0.825462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.667518</td>\n",
       "      <td>0.502689</td>\n",
       "      <td>0.933592</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.562807</td>\n",
       "      <td>1.539426</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>12.556734</td>\n",
       "      <td>0.620169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.826309</td>\n",
       "      <td>0.757590</td>\n",
       "      <td>0.936663</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.571274</td>\n",
       "      <td>2.414617</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2.399559</td>\n",
       "      <td>0.558785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.623687</td>\n",
       "      <td>0.785930</td>\n",
       "      <td>0.600911</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.082438</td>\n",
       "      <td>7.340140</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>5.032125</td>\n",
       "      <td>0.661511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.832376</td>\n",
       "      <td>0.873171</td>\n",
       "      <td>0.746166</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.341402</td>\n",
       "      <td>4.115067</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2.836248</td>\n",
       "      <td>0.701162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booster  colsample_bylevel  colsample_bynode  colsample_bytree  gamma  \\\n",
       "0        0           0.737251          0.658760          0.850624    1.1   \n",
       "1        0           0.667518          0.502689          0.933592    0.8   \n",
       "2        1           0.826309          0.757590          0.936663    8.2   \n",
       "3        0           0.623687          0.785930          0.600911    5.0   \n",
       "4        0           0.832376          0.873171          0.746166    4.0   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators  reg_lambda  subsample  \n",
       "0       0.762499   6.865682        2200.0   12.782261   0.825462  \n",
       "1       0.562807   1.539426        2200.0   12.556734   0.620169  \n",
       "2       0.571274   2.414617        1600.0    2.399559   0.558785  \n",
       "3       0.082438   7.340140        1600.0    5.032125   0.661511  \n",
       "4       0.341402   4.115067         900.0    2.836248   0.701162  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the search hyperparameter combinations\n",
    "\n",
    "pd.DataFrame(trials.vals).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.947247</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.964873</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.942184</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.939747</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.944747</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss status\n",
       "0 -0.947247     ok\n",
       "1 -0.964873     ok\n",
       "2 -0.942184     ok\n",
       "3 -0.939747     ok\n",
       "4 -0.944747     ok"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the results\n",
    "\n",
    "pd.DataFrame(trials.results).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bylevel</th>\n",
       "      <th>colsample_bynode</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>subsample</th>\n",
       "      <th>loss</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.711479</td>\n",
       "      <td>0.565276</td>\n",
       "      <td>0.576124</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.506087</td>\n",
       "      <td>8.824203</td>\n",
       "      <td>400.0</td>\n",
       "      <td>19.942757</td>\n",
       "      <td>0.779631</td>\n",
       "      <td>-0.927152</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.947287</td>\n",
       "      <td>0.663653</td>\n",
       "      <td>0.856963</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.147014</td>\n",
       "      <td>4.032818</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>16.771703</td>\n",
       "      <td>0.525715</td>\n",
       "      <td>-0.932215</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.596640</td>\n",
       "      <td>0.755243</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.802608</td>\n",
       "      <td>2.951827</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>14.870752</td>\n",
       "      <td>0.613264</td>\n",
       "      <td>-0.934652</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.707979</td>\n",
       "      <td>0.852354</td>\n",
       "      <td>0.538706</td>\n",
       "      <td>8.3</td>\n",
       "      <td>0.837293</td>\n",
       "      <td>5.307401</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>12.143680</td>\n",
       "      <td>0.656224</td>\n",
       "      <td>-0.937152</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.600675</td>\n",
       "      <td>0.545271</td>\n",
       "      <td>0.774166</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>5.381558</td>\n",
       "      <td>700.0</td>\n",
       "      <td>6.927344</td>\n",
       "      <td>0.751204</td>\n",
       "      <td>-0.937184</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booster  colsample_bylevel  colsample_bynode  colsample_bytree  gamma  \\\n",
       "0        0           0.711479          0.565276          0.576124    9.5   \n",
       "1        1           0.947287          0.663653          0.856963    7.4   \n",
       "2        0           0.800030          0.596640          0.755243    6.1   \n",
       "3        0           0.707979          0.852354          0.538706    8.3   \n",
       "4        0           0.600675          0.545271          0.774166    6.8   \n",
       "\n",
       "   learning_rate  max_depth  n_estimators  reg_lambda  subsample      loss  \\\n",
       "0       0.506087   8.824203         400.0   19.942757   0.779631 -0.927152   \n",
       "1       0.147014   4.032818        1900.0   16.771703   0.525715 -0.932215   \n",
       "2       0.802608   2.951827        1600.0   14.870752   0.613264 -0.934652   \n",
       "3       0.837293   5.307401        2200.0   12.143680   0.656224 -0.937152   \n",
       "4       0.077474   5.381558         700.0    6.927344   0.751204 -0.937184   \n",
       "\n",
       "  status  \n",
       "0     ok  \n",
       "1     ok  \n",
       "2     ok  \n",
       "3     ok  \n",
       "4     ok  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat([\n",
    "    pd.DataFrame(trials.vals),\n",
    "    pd.DataFrame(trials.results)],\n",
    "    axis=1,\n",
    ").sort_values(by='loss', ascending=False).reset_index(drop=True)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Hyperparam combination')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['loss'].plot()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Hyperparam combination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9674050632911392"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(trials.results)['loss'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('hyperparameter-optimization': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2b5166dc44e1c447f633fd495edf8ade93e6e9a81bfb1e7ede5f55ce3b3ba8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
